[ft_instance_hyperparameter]
max_batch_size={batch_size} ; Use for allocate the buffer
max_seq_len=2048 ; The sequence length of position embedding table, should move to model hyper-parameter
beam_width=1 ; beam width for beam search
top_k=1 ; k value for top k sampling
top_p=0 ; p value for top p sampling
temperature=0 ; Use for sampling
repetition_penalty=1.0 ; Use for sampling
presence_penalty=0.0  ; Only one of repetition_penalty and presence_penalty are allowed.
tensor_para_size={tensor_para_size}
pipeline_para_size=1
data_type=fp16
sparse=0
int8_mode=0
enable_custom_all_reduce=0
model_name={model_name}
model_dir={model_dir}
len_penalty=0.0
beam_search_diversity_rate=0.0
shared_contexts_ratio=0.0	; THIS IS VERY IMPORTANT!

[request]
request_batch_size={batch_size}    ; determine by the request
request_output_len={output_len}   ; determine by the request
return_log_probs=false  ; return the output log probs and cumulative log probs.
context_log_probs=false ; include input contexts in the cumulative log probability computation.
remove_padding=true
context_embeddings=true

[opt-1.3b]
head_num=32
size_per_head=64
vocab_size=50272
decoder_layers=24
start_id=2
end_id=2
inter_size=8192
model_variant=opt-pre ;define variant structure

[opt-6.7b]
head_num=32
size_per_head=128
vocab_size=50272
decoder_layers=32
start_id=2
end_id=2
inter_size=16384
model_variant=opt-pre ;define variant structure

[opt-13b]
head_num=40
size_per_head=128
vocab_size=50272
decoder_layers=40
start_id=2
end_id=2
inter_size=20480
model_variant=opt-pre ;define variant structure
