# Benchmark Serving

Scripts for online serving benchmark.

Scripts include:

- `run-experiment-single-server-multi-client.py`: Launch a inference server
(FastServe/VLLM/FT's) and start clients with different num-prompts and 
request-rates. It will log down the output of clients under log/{log_dir}
- `summarize-result.py`: Summarize logs generated by the script above.
- `draw-req-latency-histogram.py`: Draw histograms of request latency distribution
- `ft-api-server.py`: An api server for FasterTransformer. This api server provides similar
apis with FastServe's and VLLM's api server. It can be used for online inference
and benchmarking of FasterTransformer. This script uses MPI to launch multiple processes. Please prepend `mpirun -n XXX`
before the command to launch the script. `XXX` is the number of processes. Currently this script only supports tensor parallelism. It does not support 
pipeline parallelism.
- `benchmark-serving.py`: A load generator. Act as the client when benchmarking.

## Online Serving benchmark
You should first download the ShareGPT dataset. 
Then launch the api server with corresponding serving backend:
```bash
    # (FastServe backend)
    python -m fastserve.api_server.fastserve_api_server \
        --model <your_model>

    # (vLLM backend)
    python -m vllm.entrypoints.api_server \
        --model <your_model> --swap-space 16 \
        --disable-log-requests

    # (TGI backend)
    ./launch_hf_server.sh <your_model>

    # (FasterTransformer backend)
    mpirun -n <tensor-para-size> python ft-api-server.py --port <port> \
        --model-name <model-name> \
        --tensor-para-size <tensor-para-size> \
        --max-batch-size <max-batch-size> \
        --vocab-file <path/to/vocab.json> \
        --merge-file <path/to/merge.txt> \
        --lib-path <path/to/libth_transformer.so> \
        --max-tokens-per-batch <max-tokens-per-batch>

```
Then run the benchmark script which serves as the client side:
```bash
    python benchmarks/benchmark_serving.py \
        --backend <backend> \
        --tokenizer <your_model> \
        --dataset <target_dataset> \
        --request-rate <request_rate> \
        --process-name <process_name> \
```